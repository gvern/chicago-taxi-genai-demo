# Configuration for Custom Training + HPT Pipeline

gcp:
  project_id: "avisia-certification-ml-yde" # Replace with your project ID
  region: "europe-west1" 
  staging_bucket: "gs://your-gcs-bucket-name/staging" # IMPORTANT: Replace with your GCS bucket URI

bigquery:
  dataset_id: "chicago_taxis" # Dataset for pipeline outputs (training table, prediction input/output)
  source_table_id: "bigquery-public-data.chicago_taxi_trips.taxi_trips" # Source for raw data (used by SQL query)
  training_table_name: "demand_by_hour" # Name for the generated training table
  prediction_input_table_name: "forecast_input" # Name for the generated prediction input table
  prediction_output_prefix: "forecast_output" # Prefix for batch prediction output table

# Forecasting Configuration
forecasting:
  time_column: timestamp_hour
  target_column: trip_count
  context_column: pickup_community_area
  time_granularity: "hour"
  forecast_horizon: 24
  window_size: 168              # Taille du contexte historique utilisé
  available_at_forecast:        # Toutes ces colonnes sont connues à l'avance (calculables pour l'avenir)
    - timestamp_hour
    - day_of_year
    - day_of_week
    - hour
    - month
    - is_weekend
    - hour_sin
    - hour_cos
    - year
    - pickup_community_area
  unavailable_at_forecast:      # Connue uniquement dans le passé
    - trip_count
  time_window:
    end_date: "2023-11-22" # Date de fin souhaitée pour les données d'entraînement/test
    max_data_points: 2950 
data_preprocessing:
  sql_template_path: "src/pipelines/components/preprocessing/bigquery_queries.sql"
  time_window:
    end_date_str: "2023-11-22" # End date for training data generation
    max_data_points: 2950 # Max number of hours per series for training data


custom_training:
  # Static arguments passed to the training script (train_xgboost_hpt.py)
  static_args:
    time_column: "timestamp_hour"
    target_column: "trip_count"
    series_id_column: "pickup_community_area"
    feature_columns: # List of features used by the model
      - "hour"
      - "day_of_week"
      - "month"
      - "year"
      - "day_of_year"
      - "week_of_year"
      - "is_weekend"
      - "hour_sin"
      - "hour_cos"
      - "pickup_community_area"
      - "is_holiday" # Add if generated and used
    train_ratio: 0.85 # Fraction for train/validation split within the script

  # Worker pool specification for the training job (run by HPT)
  worker_pool_spec:
    machine_type: "n1-standard-4" # Machine type for training
    # IMPORTANT: Replace with the URI of your custom training container image in Artifact Registry
    container_uri: "europe-west1-docker.pkg.dev/your-project-id/your-repo-name/your-image-name:latest"
    replica_count: 1 # Must be 1 for HPT master worker

hyperparameter_tuning:
  display_name_prefix: "hpt_xgboost_taxi"
  metric_tag: "rmse" # Metric reported by the training script
  metric_goal: "MINIMIZE" # Goal for the metric (MINIMIZE or MAXIMIZE)
  max_trial_count: 20 # Total number of trials
  parallel_trial_count: 4 # Number of trials to run in parallel
  search_algorithm: "DEFAULT" # "RANDOM_SEARCH", "GRID_SEARCH", "BAYESIAN_OPTIMIZATION"

  # Parameter specification for HPT
  parameter_spec:
    learning_rate:
      parameter_id: "learning_rate"
      type: "DOUBLE"
      scale_type: "UNIT_LOG_SCALE"
      min_value: 0.01
      max_value: 0.3
    n_estimators:
      parameter_id: "n_estimators"
      type: "INTEGER"
      scale_type: "UNIT_LINEAR_SCALE"
      min_value: 50
      max_value: 500
    max_depth:
      parameter_id: "max_depth"
      type: "INTEGER"
      scale_type: "UNIT_LINEAR_SCALE"
      min_value: 3
      max_value: 10
    subsample:
      parameter_id: "subsample"
      type: "DOUBLE"
      scale_type: "UNIT_LINEAR_SCALE"
      min_value: 0.5
      max_value: 1.0
    colsample_bytree:
      parameter_id: "colsample_bytree"
      type: "DOUBLE"
      scale_type: "UNIT_LINEAR_SCALE"
      min_value: 0.5
      max_value: 1.0
    reg_lambda:                   # Régularisation L2
      parameter_id: "reg_lambda" 
      type: "DOUBLE"
      scale_type: "UNIT_LOG_SCALE" # Echelle log car effet sur ordres de grandeur
      min_value: 0.1
      max_value: 10.0
batch_prediction:
  job_display_name_prefix: "batch_pred_taxi"
  # Optional: Specify machine type if needed, otherwise defaults are used
  # machine_type: "n1-standard-4"
  # Optional: Specify starting/max replica count if needed
  # starting_replica_count: 1
  # max_replica_count: 5
  generate_explanation: false # Whether to generate explanations

# Optional: Model Registry configuration
model_registry: 
  model_display_name_prefix: "chicago_taxi_forecast_model"
  # If the training script saves the model, specify the serving container
  # serving_container_image_uri: "europe-west1-docker.pkg.dev/your-project-id/your-repo-name/your-serving-image:latest"

# Optional: Parameters for generate_forecast_input.py if run separately or via KFP component
forecast_input_generation:
  horizon_hours: 24 # Number of future hours to generate input data for
  forecast_start_time: "2023-11-22T00:00:00Z" # Start time for the forecast
  forecast_horizon_hours: 24 # Number of hours to forecast



    # Vertex AI Forecast Model Parameters
#vertex_ai_forecast:
  #display_name: "chicago_taxi_forecast_model"
  #optimization_objective: "minimize-rmse"
  #budget_milli_node_hours: 1000    # ~1h d'entraînement
  #column_transformations:
    #timestamp_hour: "timestamp"
    #pickup_community_area: "categorical"
    #trip_count: "numeric"
    #hour: "numeric"
    #day_of_week: "categorical"
    #month: "categorical"
    #day_of_year: "numeric"
    #is_weekend: "categorical"
    #hour_sin: "numeric"
    #hour_cos: "numeric"
    #year: "numeric"