# üìÑ Whitepaper ‚Äì Forecasting de la Demande de Taxis √† Chicago

## üéØ Executive Summary

Ce projet propose une solution de pr√©vision horaire du volume de trajets de taxi dans la ville de Chicago, √† l‚Äô√©chelle de chaque quartier (`pickup_community_area`). Bas√© sur le service Vertex AI Forecast de Google Cloud, le pipeline permet d‚Äôanticiper la demande pour optimiser l‚Äôallocation des taxis, r√©duire les temps d‚Äôattente des clients et mieux r√©pondre aux pics de trafic ou √©v√©nements.

La solution utilise BigQuery pour l‚Äôagr√©gation √† grande √©chelle du dataset public Chicago Taxi Trips (187M+ lignes), int√®gre les meilleures pratiques de scalabilit√©, de mod√©lisation temporelle, et s'appuie sur une architecture moderne GCP de bout-en-bout.

## üåê Data Locations

This project utilizes the following data assets stored on Google Cloud:

*   **Project ID:** `avisia-certification-ml-yde`
*   **Region (Primary):** `us-central1` (used for Vertex AI resources and GCS bucket)
*   **Source Data:** BigQuery Public Dataset `bigquery-public-data.chicago_taxi_trips.taxi_trips`.
*   **Processed Data Location (BigQuery):** The final table used for training, generated by the pipeline's preprocessing step: `bq://avisia-certification-ml-yde.chicago_taxis.demand_by_hour` (in BigQuery's US multi-region location).
*   **Vertex AI Artifacts Location (GCS):** Bucket used for Vertex AI Pipeline root, model artifacts, and staging: `gs://avisia-certification-ml-yde-vertex-bucket`.
*   **Vertex AI Datasets:** TimeSeriesDatasets created by the pipeline, visible in the Vertex AI UI under the specified project and region.
*   **Vertex AI Models:** Trained AutoML Forecasting models registered in the Vertex AI Model Registry under the specified project and region.

---

## üíº Business Goal

L‚Äôobjectif m√©tier principal est de permettre √† l‚Äôop√©rateur de taxis de :

- **Pr√©voir le volume de demandes par heure et par quartier**
- **Optimiser la r√©partition de la flotte de v√©hicules**
- **R√©duire les d√©lais d‚Äôattente et les trajets √† vide**
- **Mieux g√©rer les jours f√©ri√©s, √©v√©nements, et p√©riodes de forte demande**

---

## üîß ML Use Case: Time Series Forecasting multi-s√©ries

- **Type de t√¢che :** Pr√©vision de s√©ries temporelles (Forecasting)
- **Variable cible :** `trip_count` (nombre de trajets par heure)
- **Identifiant de s√©rie :** `pickup_community_area`
- **Granularit√© :** Horaire (`timestamp_hour`)
- **Horizon de pr√©vision :** 24h (configurable)
- **Approche :** Mod√®le personnalis√© **XGBoost** entra√Æn√© sur Vertex AI avec optimisation d'hyperparam√®tres (Hypertune).
- **M√©triques de r√©f√©rence :** RMSE, MAE, MAPE (WAPE, MASE)

---

## üìä Data Exploration & Feature Engineering

### üîç Exploration

- Analyse des s√©ries temporelles sur 1 an (tendances, saisonnalit√©)
- D√©tection des zones √† forte variabilit√© de demande
- Analyse des heures de pointe, jours f√©ri√©s, jours de la semaine

### üõ†Ô∏è Feature Engineering

- Agr√©gation BQ par `pickup_community_area √ó timestamp_hour`
- G√©n√©ration des s√©ries compl√®tes via `GENERATE_TIMESTAMP_ARRAY`
- Encodage temporel :
  - `hour`, `day_of_week`, `month`, `is_weekend`, `is_holiday`
- Int√©gration exog√®ne :
  - Donn√©es m√©t√©o (via API NOAA, optionnel)
  - Calendrier des √©v√©nements publics (optionnel)
- Cr√©ation **manuelle** des lags et fen√™tres glissantes (via script Python/Pandas).
- Encodage cyclique des features temporelles (sin/cos).
- Encodage One-Hot des identifiants de s√©rie (`pickup_community_area`).

### ML.3.4.3.3 Feature Engineering

Feature engineering was primarily performed within BigQuery to leverage its scalability for processing the large dataset. The goal was to create a structured time series dataset suitable for training a forecasting model. Additional feature engineering steps specific to the XGBoost model are performed in Python during the training and prediction phases.

**Key Steps:**

1.  **Aggregation (BigQuery):** Raw trip data was aggregated to calculate the hourly `trip_count` for each `pickup_community_area`.
2.  **Time Index Generation (BigQuery):** A complete sequence of hourly timestamps covering the entire period of the raw data was generated.
3.  **Cartesian Product & Filling (BigQuery):** A Cartesian product between all unique community areas and all hourly timestamps was created. This ensures a complete grid. The aggregated `trip_count` was then joined, filling missing combinations (hours/zones with no trips) with a `trip_count` of 0.
4.  **Basic Temporal Feature Creation (BigQuery):** Standard temporal features were extracted directly from the `timestamp_hour`: `hour`, `day_of_week`, `month`, `year`, `day_of_year`.
5.  **Derived Temporal Features (BigQuery):** Simple binary flags like `is_weekend` and `is_holiday` (based on common US holidays) were created.
6.  **Lag/Window & Advanced Features (Python/Pandas):** Features specifically beneficial for tree-based models like XGBoost are generated in Python scripts (`src/pipelines/components/preprocessing/feature_engineering.py`) before training and prediction. This includes:
    *   **Lag Features:** Past values of the target variable (`trip_count`).
    *   **Rolling Window Features:** Statistics (e.g., mean) calculated over rolling time windows of past target values.
    *   **Cyclical Features:** Sine and cosine transformations of temporal features (`hour`, `day_of_week`) to capture cyclical patterns smoothly.
    *   **One-Hot Encoding:** Conversion of the categorical `pickup_community_area` into numerical features.

**Final Features Provided to XGBoost Model (Illustrative):**
*   `hour`, `day_of_week`, `month`, `year`, `day_of_year`, `is_weekend`, `is_holiday` (From BigQuery)
*   `target_lag_1`, `target_lag_2`, ... (From Python)
*   `target_rollmean_3`, `target_rollmean_6`, ... (From Python)
*   `hour_sin`, `hour_cos`, `dow_sin`, `dow_cos` (From Python)
*   `area_1`, `area_2`, ..., `area_77` (One-Hot Encoded `pickup_community_area` from Python)
*   *(Note: `timestamp_hour` and `pickup_community_area` are used for joining/grouping but typically not directly as features in the final model matrix)*

These features provide the model with essential information about the time context, recent history, and categorical identity for each observation.

**Code Snippets Example (Illustrative from `bigquery_queries.sql`):**

*Generating all combinations and filling counts:*
```sql
-- 4. Generate Cartesian product (hour x zone)
all_combinations AS (
  SELECT
    h.timestamp_hour,
    a.pickup_community_area
  FROM all_hours h -- CTE generating all hourly timestamps
  CROSS JOIN areas a -- CTE with distinct pickup_community_area
),
-- 5. Count aggregated trips per hour x zone
aggregated AS (
  SELECT
    TIMESTAMP_TRUNC(trip_start_timestamp, HOUR) AS timestamp_hour,
    pickup_community_area,
    COUNT(*) AS trip_count
  FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`
  WHERE pickup_community_area IS NOT NULL
  GROUP BY 1, 2
),
-- 6. Join all combinations with aggregated data, fill missing with 0
filled AS (
  SELECT
    ac.timestamp_hour,
    ac.pickup_community_area,
    IFNULL(agg.trip_count, 0) AS trip_count
  FROM all_combinations ac
  LEFT JOIN aggregated agg
    ON ac.timestamp_hour = agg.timestamp_hour
   AND ac.pickup_community_area = agg.pickup_community_area
)
```

*Extracting temporal features:*
```sql
-- 7. Add temporal features
SELECT
  timestamp_hour,
  pickup_community_area,
  trip_count,
  EXTRACT(HOUR FROM timestamp_hour) AS hour,
  EXTRACT(DAYOFWEEK FROM timestamp_hour) AS day_of_week, -- Note: BQ specific day numbering
  EXTRACT(MONTH FROM timestamp_hour) AS month,
  EXTRACT(YEAR FROM timestamp_hour) AS year,
  EXTRACT(DAYOFYEAR FROM timestamp_hour) AS day_of_year,
  IF(EXTRACT(DAYOFWEEK FROM timestamp_hour) IN (1, 7), 1, 0) AS is_weekend, -- Adjust if necessary
  IF(FORMAT_DATE('%m-%d', DATE(timestamp_hour)) IN ('01-01', '07-04', '12-25'), 1, 0) AS is_holiday -- Simple heuristic
FROM filled
ORDER BY timestamp_hour, pickup_community_area;
```
(Full query: `src/data_preprocessing/bigquery_queries.sql`)

### ML.3.4.3.2 Data Exploration

**Process:**
Initial data exploration was crucial to understand the structure, quality, and characteristics of the raw Chicago Taxi Trips data. This involved:

1.  **Sampling:** Due to the large volume (187M+ rows), initial exploration was performed on a representative sample obtained directly from BigQuery.
2.  **Descriptive Statistics:** Calculating basic statistics (count, mean, min, max, quartiles) for key numerical columns like `trip_miles`, `trip_seconds`.
3.  **Distribution Analysis:** Visualizing the distribution of trips over time (years, months, days, hours), trip duration, trip distance, and fares.
4.  **Categorical Analysis:** Examining the distribution of pickups and dropoffs across community areas.
5.  **Time Series Visualization:** Plotting the aggregated hourly demand per community area to identify trends, seasonality (daily, weekly, yearly), and potential outliers or anomalies.
6.  **Correlation Analysis:** Investigating potential relationships between demand and temporal features (hour, day of week, holidays).

**Tools Used:**
*   Google BigQuery (for sampling and initial aggregation)
*   Vertex AI Workbench (Jupyter environment)
*   Python Libraries:
    *   `google-cloud-bigquery` (to interact with BigQuery)
    *   `pandas` (for data manipulation and analysis)
    *   `matplotlib` & `seaborn` (for visualization)

**Key Findings & Influence on Decisions:**
*   **Strong Seasonality:** Clear daily and weekly patterns were observed in taxi demand, confirming the need for time-based features.
*   **Varying Zone Behavior:** Different community areas exhibited significantly different demand patterns and volumes, justifying a multi-time-series approach (forecasting per `pickup_community_area`).
*   **Data Completeness:** While generally good, some fields like `pickup_community_area` had null values that needed filtering.
*   **Outliers:** Some trips had unusually long durations or distances, which were considered but ultimately included as they might represent valid edge cases or data entry issues that the model should be robust to (or handled by BQ aggregation).
*   **Need for Full Time Index:** Simple aggregation revealed gaps in the time series (hours with zero trips for a given zone). This highlighted the necessity of creating a complete time index (all hours for all zones) and filling missing values (with 0 trips) during preprocessing, which is handled in the `bigquery_queries.sql`.

**Code Snippets Example (Illustrative):**

*Fetching a sample from BigQuery:*
```python
# In a Vertex AI Workbench Notebook
from google.cloud import bigquery
import pandas as pd

client = bigquery.Client(project='avisia-certification-ml-yde')

# Query to get a recent sample
sample_query = f"""
SELECT
    trip_start_timestamp,
    pickup_community_area,
    trip_miles,
    trip_seconds
FROM
    `bigquery-public-data.chicago_taxi_trips.taxi_trips`
WHERE
    pickup_community_area IS NOT NULL
    AND trip_start_timestamp >= '2022-01-01' # Example filter
LIMIT 100000
"""
sample_df = client.query(sample_query).to_dataframe()

print(sample_df.info())
print(sample_df.describe())
```

*Basic Time Series Plotting (after aggregation):*
```python
# Assuming 'hourly_demand_df' is a pandas DataFrame with
# index='timestamp_hour', columns='pickup_community_area', values='trip_count'
import matplotlib.pyplot as plt
import seaborn as sns

# Select a specific area to plot
area_to_plot = 6 # Example area
demand_subset = hourly_demand_df[area_to_plot].loc['2022-01-01':'2022-01-31'] # Example date range

plt.figure(figsize=(15, 6))
sns.lineplot(data=demand_subset)
plt.title(f'Hourly Taxi Demand - Area {area_to_plot} (Jan 2022)')
plt.xlabel('Timestamp')
plt.ylabel('Trip Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```
(Detailed exploration notebooks: `notebooks/1_EDA.ipynb`, `notebooks/eda_chicago_taxi_v3_bigquery.ipynb`)

### ML.3.4.3.4 Preprocessing and the Data Pipeline

**Pipeline Overview:**
The primary data preprocessing pipeline is implemented as a single, comprehensive BigQuery SQL query (`src/data_preprocessing/bigquery_queries.sql`). This query transforms the raw taxi trip data into the final `demand_by_hour` table, ready for consumption by Vertex AI Forecast.

**Orchestration:**
This BigQuery-based preprocessing step is orchestrated as part of a larger MLOps workflow using **Vertex AI Pipelines** (built on Kubeflow Pipelines). A dedicated KFP component (`src/pipelines/components/run_bq_forecasting_query.py`) is responsible for executing the BigQuery query.

**Why BigQuery?**
BigQuery was chosen for preprocessing due to:
*   **Scalability:** Easily handles the large volume of the Chicago Taxi dataset without requiring manual cluster management.
*   **SQL Interface:** Allows complex aggregations, joins, and feature engineering logic to be expressed declaratively in SQL.
*   **Integration:** Seamlessly integrates with Vertex AI Datasets, allowing Vertex AI Forecast to read directly from the resulting BigQuery table.

*(Alternative: While a Dataflow pipeline (`src/data_preprocessing/dataflow_pipeline.py`) exists in the repository for potential row-level processing, the primary path for this forecasting demo relies solely on BigQuery for the necessary aggregation and feature engineering.)*

**Callable API/Function:**
The preprocessing step is made "callable" within the Vertex AI Pipeline through the KFP component. This component takes parameters like `project_id`, `location`, `dataset_id`, and `destination_table` and executes the embedded SQL query.

**Code Snippets Example:**

*Core Aggregation Logic Snippet (from `src/data_preprocessing/bigquery_queries.sql`):*
```sql
-- Simplified view of the core transformation within the CTE structure
CREATE OR REPLACE TABLE `{project_id}.{dataset_id}.{destination_table}` AS
WITH
  -- (CTEs for generating all_hours and all_areas)
  all_combinations AS (...),
  aggregated AS (
    SELECT
      TIMESTAMP_TRUNC(trip_start_timestamp, HOUR) AS timestamp_hour,
      pickup_community_area,
      COUNT(*) AS trip_count
    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`
    WHERE pickup_community_area IS NOT NULL
    GROUP BY 1, 2
  ),
  filled AS (
    SELECT
      ac.timestamp_hour,
      ac.pickup_community_area,
      IFNULL(agg.trip_count, 0) AS trip_count
    FROM all_combinations ac
    LEFT JOIN aggregated agg USING(timestamp_hour, pickup_community_area)
  )
-- Final SELECT adding temporal features (hour, day_of_week, etc.)
SELECT ... FROM filled ORDER BY ...;
```

*KFP Component Invocation Snippet (from `src/pipelines/forecasting_pipeline.py`):*
```python
from src.pipelines.components.run_bq_forecasting_query import run_bq_forecasting_query
# ... other imports and pipeline definition ...

@dsl.pipeline(...)
def forecasting_pipeline(
    project: str,
    location: str,
    bq_output_uri: str,
    # ... other parameters
):
    # Step 1: Generate the final BigQuery table using the component
    prep_step = run_bq_forecasting_query(
        project_id=project,
        location=location,
        # dataset_id and destination_table use component defaults or can be passed
        # The component returns the table URI upon completion
    )
    # The output table BQ_URI (e.g., bq://project.dataset.table)
    # is then used as input for the next step (creating Vertex AI Dataset)
```
This demonstrates how the SQL logic is encapsulated and invoked programmatically within the pipeline.

### ML.3.4.3.5 Machine Learning Model Design(s) and Selection

**Model Selected:** Custom **XGBoost** Model trained on Vertex AI

**Rationale for Selection:**
XGBoost (Extreme Gradient Boosting) is an excellent choice for this task of forecasting taxi demand for several reasons, making it a good candidate for the Google demo:

*   **Performance on Tabular Data:** XGBoost excels on structured/tabular data, which is exactly what we get after transforming the raw time series into a feature-rich table (temporal features, lags, rolling averages, categorical indicators like zone). It effectively captures complex, non-linear relationships between these features and the target (number of trips).
*   **Efficient Feature Management:**
    *   It can handle a large number of features, useful as feature engineering can generate many (especially with lags, rolling averages, and one-hot encoding of zones).
    *   It provides feature importance metrics (`feature_importance_`), helping understand which factors most influence demand and guiding model improvement or feature selection.
*   **Built-in Regularization:** XGBoost natively includes L1 (Lasso) and L2 (Ridge) regularization via `reg_alpha` and `reg_lambda` parameters. This is crucial for preventing overfitting, especially with potentially noisy data and many correlated features (like lags and rolling averages).
*   **Speed and Scalability:** Implemented in C++ and optimized for performance (parallel computation, efficient memory management, optimized tree algorithms). While extreme scalability might not be necessary for this specific dataset, its speed allows faster iteration and hyperparameter tuning within a reasonable time.
*   **Missing Value Handling:** XGBoost can natively handle missing values, although clean imputation beforehand is often preferable.
*   **Flexibility and Robustness:** Offers numerous hyperparameters for fine-tuning. Gradient boosting itself is robust and often yields state-of-the-art results for this type of problem compared to simpler linear models or even traditional statistical approaches (like ARIMA) which may handle exogenous features less effectively.
*   **Popularity and Support:** It's a very popular, well-documented library with a large community, facilitating problem-solving and applying best practices.

In summary, for a forecasting problem where the time series is transformed into a rich tabular dataset, XGBoost offers an excellent balance between predictive performance, training speed, robustness against overfitting, and relative interpretability (via feature importance), making it highly relevant for the demo.

**Alternative Models Considered (Briefly):**
*   **Vertex AI Forecast (AutoML):** A powerful automated solution, but using a custom model like XGBoost allows for more granular control over feature engineering and model architecture, demonstrating different Vertex AI capabilities (Custom Training, Hypertune).
*   **Other Custom Models (e.g., LSTM, LightGBM):** LSTMs might be overly complex for this tabular structure post-feature engineering. LightGBM is a strong alternative to XGBoost, often faster but sometimes slightly less accurate.
*   **Traditional Statistical Models (ARIMA, Prophet):** Might struggle with the large number of series and the effective incorporation of numerous exogenous features compared to gradient boosting.

**Code Snippet Example (Illustrative - KFP Component Call for Custom Training Job):**

*Launching the Custom Training Job with Hypertune (conceptual, based on `src/pipelines/components/model_training/launch_hpt_job.py`):*
```python
# Within the KFP pipeline definition (forecasting_pipeline.py)
from src.pipelines.components.model_training.launch_hpt_job import launch_hpt_job

# ... define HPT config, worker spec, static args ...

launch_hpt_op = launch_hpt_job(
    project=project,
    location=location,
    staging_bucket=staging_bucket,
    display_name_prefix=hpt_display_name_prefix, # e.g., "xgboost-hpt"
    worker_pool_spec=worker_full_spec, # Defines container and machine type
    hpt_config=hpt_full_config, # Defines HPT metric, goal, params, algo
    static_args=static_script_args, # Args passed to the training script
    training_data_uri=prepare_data_op.outputs["destination_table_uri"] # Input data
)
# The component launches a Vertex AI HyperparameterTuningJob,
# which manages multiple CustomJob trials running the XGBoost training script.
```
This snippet shows how a custom training job, potentially wrapped in a hyperparameter tuning job, is launched via a KFP component.

### ML.3.4.3.6 Machine Learning Model Training and Development

Model training is performed using **Vertex AI Custom Training**, potentially orchestrated by **Vertex AI Hyperparameter Tuning (Hypertune)**, all managed via a Vertex AI Pipeline.

**Training Environment:** Vertex AI Custom Training (using pre-built or custom containers)

**Key Aspects:**

*   **Data Splitting (Sampling):** Data splitting is performed **manually within the custom training script** (`src/pipelines/components/model_training/train_xgboost_hpt.py`). A chronological split is typically used, separating the data loaded from BigQuery into training and validation sets based on timestamps (e.g., using `train_end_date` and `val_end_date`). This ensures the model is validated on data points that occur *after* the training data.
    *   *Code Snippet (Illustrative - From `train_xgboost_hpt.py`):*
        ```python
        # ... load data into df ...
        # Apply feature engineering
        df = preprocess_data_for_xgboost(df, is_train=True)
        X, y = df.drop(columns=[target_col]), df[target_col]

        # Split based on time column
        train_mask = pd.to_datetime(df[time_col]) < pd.to_datetime(train_end_date)
        val_mask = (pd.to_datetime(df[time_col]) >= pd.to_datetime(train_end_date)) & \
                   (pd.to_datetime(df[time_col]) < pd.to_datetime(val_end_date))
        X_train, y_train = X[train_mask], y[train_mask]
        X_val, y_val = X[val_mask], y[val_mask]

        # Train XGBoost model
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], ...)
        ```
*   **Implementation & GCP Best Practices:**
    *   **Vertex AI SDK & Pipelines:** Training is invoked programmatically using the `google-cloud-aiplatform` SDK within KFP components (`launch_hpt_job.py`), facilitating MLOps automation and reproducibility.
    *   **Custom Training Jobs:** Training runs as a Vertex AI Custom Job, executing the provided Python script (`train_xgboost_hpt.py`) within a specified container environment (e.g., `worker_container_uri`).
    *   **Artifact Management:** The trained model (`model.xgb`) is saved to a GCS location specified by the environment variable `AIP_MODEL_DIR`, managed by Vertex AI. The path to the best model from HPT is an output of the `launch_hpt_op` component.
    *   **Monitoring:** Training progress (logs) and resource consumption can be monitored via the Google Cloud Console (Vertex AI -> Training -> Custom Jobs / Hyperparameter Tuning Jobs).
    *   *Code Snippet:* The KFP component invocation shown in the previous section (ML.3.4.3.5) represents the core implementation step following GCP best practices for managed custom ML training.
*   **Evaluation Metric:** The primary metric used for optimization during HPT is **Root Mean Squared Error (RMSE)** (specified in `hpt_config` and reported by the training script using the `hypertune` library).
    *   **Rationale:** RMSE heavily penalizes large prediction errors, which is important for avoiding significant underestimation of taxi demand during peak hours.
*   **Hyperparameter Optimization (HPO):** Vertex AI Hypertune is used to find the best hyperparameters for the XGBoost model. It systematically runs multiple training trials (Custom Jobs) with different hyperparameter combinations (e.g., `n_estimators`, `learning_rate`, `max_depth`) specified in `hpt_config`. The training script (`train_xgboost_hpt.py`) calculates the RMSE on the validation set and reports it back to Hypertune using the `hypertune` library. Hypertune then selects the trial with the best metric value.
    *   *Code Snippet (Illustrative - Reporting metric in `train_xgboost_hpt.py`):*
        ```python
        # ... train model and calculate rmse on validation set ...
        import hypertune

        hpt = hypertune.HyperTune()
        hpt.report_hyperparameter_tuning_metric(
            hyperparameter_metric_tag='rmse', # Must match hpt_config
            metric_value=rmse,
            # global_step=EPOCHS # Optional: if reporting per epoch
        )
        ```
*   **Bias/Variance Optimization:**
    *   **Variance Control (Overfitting):** The use of a separate validation set for early stopping (`early_stopping_rounds` in `model.fit`) and for Hypertune's metric reporting helps select models/hyperparameters that generalize well, mitigating overfitting. XGBoost's built-in regularization (`reg_alpha`, `reg_lambda`) also helps.
    *   **Bias Assessment (Underfitting):** If validation RMSE remains high across many HPT trials, it might indicate the model complexity (e.g., `max_depth`) is too constrained or the features are insufficient. Hypertune explores different complexities to find a good balance.
    *   Hypertune, combined with validation set monitoring within each trial, aims to find a model that balances bias and variance effectively.

### ML.3.4.3.7 Machine Learning Model Evaluation

**Evaluation Process:**
Model evaluation occurs **within the custom training script** (`train_xgboost_hpt.py`) during each Hypertune trial. The script calculates metrics on the **validation set** after training the model with a specific set of hyperparameters. The primary metric (RMSE) is reported back to Vertex AI Hypertune to guide the search for the best model.

Final evaluation on a separate **test set** (if defined and split in the script, though the current example focuses on train/validation for HPT) would typically be done *after* HPT has identified the best hyperparameters, potentially in a separate pipeline step or at the end of the best trial's script.

**Retrieving Metrics:**
1.  **Hypertune Results:** The primary metric (RMSE) for each trial and the best trial's results are visible in the Google Cloud Console (Vertex AI -> Training -> Hyperparameter Tuning Jobs -> [Your Job Name]).
2.  **Training Job Logs:** The training script (`train_xgboost_hpt.py`) prints metrics (like validation RMSE) to the logs, which can be viewed in the Cloud Console under the specific Custom Job (trial) details.
3.  **Pipeline Outputs:** The KFP component `launch_hpt_job` outputs the GCS path to the best model found by Hypertune. Evaluation metrics could potentially be saved as pipeline artifacts if the training script is modified to do so.

**Key Evaluation Metrics (Calculated in Script):**
*   **RMSE (Root Mean Squared Error):** Primary metric for HPT. Calculated on the validation set.
*   **MAE (Mean Absolute Error):** Can be calculated alongside RMSE in the script for additional insight.
*   *(Other metrics like MAPE, WAPE, R¬≤ could also be calculated within the script if desired)*

**Example Evaluation Results (Illustrative - From Best HPT Trial):**
*(These values should be populated from your actual Vertex AI Hypertune Job results or logs)*

| Metric          | Value (Validation Set) | Source         |
| :-------------- | :--------------------- | :------------- |
| RMSE            | 12.5                   | Hypertune/Logs |
| MAE             | 8.2                    | Logs (if added) |
| *(Other)*       | ...                    | Logs (if added) |

*(Include a brief interpretation, e.g., "The best model found by Hypertune achieved an RMSE of 12.5 on the validation set...")*

**Code Snippet Example (Illustrative - Calculating & Printing Metrics in `train_xgboost_hpt.py`):**
```python
# ... after model.fit() ...
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import hypertune # Already imported for reporting

print("Evaluating model...")
predictions = model.predict(X_val)
rmse = np.sqrt(mean_squared_error(y_val, predictions))
mae = mean_absolute_error(y_val, predictions) # Example: Calculate MAE too

print(f"Validation RMSE: {rmse}")
print(f"Validation MAE: {mae}") # Print MAE to logs

print("Reporting metric to Hypertune...")
hpt = hypertune.HyperTune()
hpt.report_hyperparameter_tuning_metric(
    hyperparameter_metric_tag='rmse', # Must match hpt_config
    metric_value=rmse
)

# (Optional: Could save detailed metrics to GCS here)
```
This snippet shows how metrics are calculated on the validation set within the script and how the primary one is reported for HPT.

---

## üèóÔ∏è Data Pipeline & Preprocessing

### üîó Source

- Dataset : `bigquery-public-data.chicago_taxi_trips.taxi_trips`
- Pipeline SQL dans `bigquery_queries.sql`
- Table finale : `avisia-certification-ml-yde.chicago_taxis.demand_by_hour`

### ‚öôÔ∏è √âtapes du pipeline

1. Troncature √† l‚Äôheure (`TIMESTAMP_TRUNC`)
2. Agr√©gation : `COUNT(*) AS trip_count`
3. S√©ries compl√®tes via CROSS JOIN + LEFT JOIN
4. Remplissage des valeurs manquantes
5. Enrichissement temporel
6. Export possible vers GCS si besoin

---

## ü§ñ Model Development ‚Äì Custom XGBoost on Vertex AI

### üì¶ Data Loading & Preprocessing (Python)

- Data loaded from BigQuery table (`demand_by_hour`) into Pandas DataFrame within the training script.
- Feature Engineering (`preprocess_data_for_xgboost`):
  - Applied within the script after loading data.
  - Creates lags, rolling means, cyclical features, one-hot encoding.
  - Handles `NaN` values resulting from lags (drops for training, kept for prediction).
- Data split chronologically into Training and Validation sets.

### üöÄ Entra√Ænement (Vertex AI Custom Job + Hypertune)

- Script : `src/pipelines/components/model_training/train_xgboost_hpt.py`
- Mod√®le : `xgboost.XGBRegressor`
- HPO : Vertex AI Hypertune searches optimal XGBoost parameters (`n_estimators`, `learning_rate`, `max_depth`, etc.) based on `pipeline_config.yaml`.
- Environnement : Container specified in `pipeline_config.yaml` (`worker_container_uri`).
- Objectif HPO : `minimize-rmse` sur le jeu de validation.
- R√©sultat : Meilleur mod√®le XGBoost (`model.xgb`) sauvegard√© sur GCS.

### üìà √âvaluation

- Effectu√©e dans le script d'entra√Ænement sur le jeu de validation.
- M√©trique principale (RMSE) report√©e √† Hypertune.
- M√©triques additionnelles (MAE, etc.) visibles dans les logs du job d'entra√Ænement.

---

## ü§ñ Deployment ‚Äì Batch Prediction Strategy

- Usage de `Vertex AI Batch Prediction` avec un **mod√®le XGBoost personnalis√©**.
- Donn√©es d‚Äôentr√©e : Fichier CSV sur GCS (`future_features.csv`) g√©n√©r√© par `generate_forecast_input.py`, contenant les features futures pr√©-calcul√©es (y compris lags, rolling means bas√©s sur les donn√©es historiques les plus r√©centes, features cycliques, one-hot encoding).
- Mod√®le : Le fichier `model.xgb` du meilleur mod√®le XGBoost trouv√© par Hypertune, charg√© depuis GCS.
- R√©sultats : Fichier CSV (`predictions.csv`) sur GCS contenant les pr√©dictions.
- Composant KFP : `src/pipelines/components/predictions/batch_predict_xgboost.py`

---

## üõ°Ô∏è Security & Privacy

- Donn√©es publiques (pas de PII)
- Stockage GCP s√©curis√© :
  - Bucket GCS priv√©
  - Tables BQ avec IAM restreint
- Gestion des acc√®s Vertex AI et BQ via service account
- IAM suivant le principe du moindre privil√®ge

---

## ‚úÖ R√©sultats & Impact

- Pr√©cision de pr√©vision RMSE ~ faible sur zones denses
- Diminution anticip√©e des temps d‚Äôattente client
- Allocation optimale de la flotte de taxis
- Pipeline industrialisable et reproductible

---

## üìå Architecture R√©sum√©e

BigQuery (raw) ‚îÇ ‚îú‚îÄ‚îÄ> SQL (agr√©gation horaire + FE de base) ‚îÇ ‚Üì BQ (demand_by_hour) ‚îÇ ‚îî‚îÄ‚îÄ> Vertex AI Pipeline ‚îú‚îÄ‚îÄ> Custom Training Job (Python/XGBoost + FE avanc√©e) ‚îÇ ‚îú‚îÄ‚îÄ> Hypertune ‚îÇ ‚îú‚îÄ‚îÄ> Best Model (GCS) ‚îÇ ‚îú‚îÄ‚îÄ> Generate Future Features (Python + FE avanc√©e) ‚îÇ ‚Üì GCS (future_features.csv) ‚îÇ ‚îî‚îÄ‚îÄ> Batch Prediction (using Best Model) ‚Üì GCS (predictions.csv) + Viz
---

## üìÖ Prochaines √©tapes

- Int√©gration des donn√©es m√©t√©o
- Utilisation de Looker Studio pour dashboard pr√©dictif
- D√©ploiement du pipeline KFP complet
- Enrichissement avec les donn√©es en temps r√©el (Realtime Forecast)

---

## üìÇ R√©f√©rences & Artifacts

| √âl√©ment | Chemin |
|--------|--------|
| Donn√©es BQ pr√©par√©es | `chicago_taxis.demand_by_hour` |
| Script pr√©traitement BQ | `src/pipelines/components/data_preparation/run_bq_forecasting_query.py` (et le SQL associ√©) |
| Script Feature Engineering Python | `src/pipelines/components/preprocessing/feature_engineering.py` |
| Script entra√Ænement XGBoost | `src/pipelines/components/model_training/train_xgboost_hpt.py` |
| Script g√©n√©ration features futures | `src/pipelines/components/generate_forecasting_data/generate_forecast_input.py` |
| Script pr√©diction batch | `src/pipelines/components/predictions/batch_predict_xgboost.py` |
| Configuration Pipeline | `config/pipeline_config.yaml` |
| D√©finition Pipeline KFP | `src/pipelines/forecasting_pipeline.py` |