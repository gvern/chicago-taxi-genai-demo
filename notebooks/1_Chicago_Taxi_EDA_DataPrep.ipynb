{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704a8b71",
   "metadata": {},
   "source": [
    "# 1. Analyse Exploratoire et Préparation des Données - Chicago Taxi Trips\n",
    "\n",
    "Ce notebook combine l'analyse exploratoire du dataset Chicago Taxi Trips et la préparation des données pour le forecasting. Nous allons :\n",
    "- Explorer la structure et la qualité des données\n",
    "- Analyser les distributions et tendances temporelles\n",
    "- Préparer les données agrégées pour le forecasting par heure et par zone\n",
    "- Enrichir le dataset avec des features temporelles\n",
    "- Exporter les données vers BigQuery pour l'utilisation avec Vertex AI Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c590a",
   "metadata": {},
   "source": [
    "## 1.1 Configuration et Imports\n",
    "\n",
    "Importons les bibliothèques nécessaires et initialisons la connexion à BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c944bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno  # pour visualiser les valeurs manquantes\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import bigquery\n",
    "import db_dtypes  # Pour les types de données spécifiques à BigQuery\n",
    "\n",
    "# Pour BigQuery\n",
    "from pandas_gbq import read_gbq\n",
    "\n",
    "# Configuration visuelle\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")  # style épuré et taille adaptée aux présentations\n",
    "base_palette = sns.color_palette(\"viridis\", 8)\n",
    "sns.set_palette(base_palette)\n",
    "\n",
    "# Supprimer les warnings pour une sortie plus propre\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060d0b2",
   "metadata": {},
   "source": [
    "## 1.2 Configuration du Projet\n",
    "\n",
    "Définissons les identifiants du projet GCP et les paramètres généraux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd8a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée avec succès depuis pipeline_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Configuration du projet GCP\n",
    "PROJECT_ID = \"avisia-certification-ml-yde\"  # Remplacez par votre Project ID\n",
    "REGION = \"europe-west1\"\n",
    "BQ_DATASET = \"chicago_taxis\"\n",
    "\n",
    "# Chargement de la configuration YAML si disponible\n",
    "try:\n",
    "    with open(\"../config/pipeline_config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration chargée avec succès depuis pipeline_config.yaml\")\n",
    "except Exception as e:\n",
    "    print(f\"Impossible de charger le fichier de configuration: {e}\")\n",
    "    config = {}\n",
    "\n",
    "# Initialisation du client BigQuery\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db174c21",
   "metadata": {},
   "source": [
    "# 2. Analyse Exploratoire des Données\n",
    "\n",
    "Explorons le dataset Chicago Taxi Trips pour comprendre sa structure et identifier les patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbcac4",
   "metadata": {},
   "source": [
    "## 2.1 Chargement d'un Échantillon pour Exploration\n",
    "\n",
    "Commençons par charger un échantillon limité pour l'exploration initiale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement depuis BigQuery: name 'pd' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     exit()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Nettoyage initial des noms de colonnes\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sample\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNoms des colonnes après nettoyage :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_sample\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sample' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Définition de la requête SQL pour récupérer un échantillon depuis BigQuery\n",
    "sample_query = \"\"\"\n",
    "SELECT *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE trip_start_timestamp BETWEEN '2023-01-01' AND '2023-01-07'\n",
    "LIMIT 100000\n",
    "\"\"\"\n",
    "\n",
    "# Chargement du dataset depuis BigQuery\n",
    "try:\n",
    "    df_sample = pd.read_gbq(sample_query, project_id=PROJECT_ID)\n",
    "    print(\"Dataset d'échantillon chargé avec succès depuis BigQuery.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement depuis BigQuery: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Nettoyage initial des noms de colonnes\n",
    "df_sample.columns = df_sample.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "print(\"\\nNoms des colonnes après nettoyage :\")\n",
    "print(df_sample.columns)\n",
    "\n",
    "# Aperçu des données\n",
    "print(\"\\nAperçu des données (5 premières lignes) :\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347a2fb",
   "metadata": {},
   "source": [
    "## 2.2 Comprendre les Données\n",
    "\n",
    "Analysons la structure et les statistiques descriptives des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des types de données\n",
    "# Timestamps\n",
    "for col in ['trip_start_timestamp', 'trip_end_timestamp']:\n",
    "    if col in df_sample.columns:\n",
    "        df_sample[col] = pd.to_datetime(df_sample[col], errors='coerce')\n",
    "\n",
    "# Numériques clés\n",
    "cols_numeric = ['trip_seconds', 'trip_miles', 'fare', 'tips', 'tolls', 'extras', 'trip_total']\n",
    "for col in cols_numeric:\n",
    "    if col in df_sample.columns:\n",
    "        df_sample[col] = pd.to_numeric(df_sample[col], errors='coerce')\n",
    "\n",
    "# Afficher les informations sur le dataframe\n",
    "print(\"\\nInformations générales sur le DataFrame:\")\n",
    "df_sample.info()\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "df_sample.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493c944",
   "metadata": {},
   "source": [
    "## 2.3 Analyse des Valeurs Manquantes\n",
    "\n",
    "Visualisons et quantifions les valeurs manquantes dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et tri des pourcentages de valeurs manquantes\n",
    "missing_pct = df_sample.isnull().sum() / len(df_sample) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "print(\"\\nPourcentage de valeurs manquantes par colonne (> 0%):\")\n",
    "print(missing_pct)\n",
    "\n",
    "# Visualisation du pattern des valeurs manquantes\n",
    "plt.figure(figsize=(16, 8))\n",
    "msno.matrix(df_sample.iloc[:, :40], sparkline=False, fontsize=10)\n",
    "plt.title(\"Matrice des Valeurs Manquantes\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Visualisation par barre\n",
    "if len(missing_pct) > 0:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    missing_pct.plot(kind='bar', color='salmon')\n",
    "    plt.title(\"Pourcentage de Valeurs Manquantes par Colonne (> 0%)\")\n",
    "    plt.ylabel(\"Pourcentage (%)\")\n",
    "    plt.xlabel(\"Colonnes\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a9276",
   "metadata": {},
   "source": [
    "## 2.4 Analyse des Tendances Temporelles\n",
    "\n",
    "Analysons les patterns temporels des courses de taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e881c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de features temporelles\n",
    "if 'trip_start_timestamp' in df_sample.columns:\n",
    "    df_sample['start_hour'] = df_sample['trip_start_timestamp'].dt.hour\n",
    "    df_sample['day_of_week'] = df_sample['trip_start_timestamp'].dt.dayofweek  # 0=Lundi, 6=Dimanche\n",
    "    df_sample['day_name'] = df_sample['trip_start_timestamp'].dt.day_name()  # Nom du jour\n",
    "    df_sample['month'] = df_sample['trip_start_timestamp'].dt.month\n",
    "    df_sample['is_weekend'] = df_sample['day_of_week'].isin([5, 6]).astype(int)  # 1 si weekend, 0 sinon\n",
    "    \n",
    "    # Visualisation des tendances par heure\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    hourly_count = df_sample.groupby('start_hour').size()\n",
    "    sns.barplot(x=hourly_count.index, y=hourly_count.values, color=base_palette[2])\n",
    "    plt.title(\"Distribution des Courses par Heure de la Journée\")\n",
    "    plt.xlabel(\"Heure de départ (0-23)\")\n",
    "    plt.ylabel(\"Nombre de courses\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualisation des tendances par jour de la semaine\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    sns.countplot(data=df_sample, x='day_name', order=day_order, palette=sns.color_palette(\"viridis\", 7))\n",
    "    plt.title(\"Distribution des Courses par Jour de la Semaine\")\n",
    "    plt.xlabel(\"Jour de la semaine\")\n",
    "    plt.ylabel(\"Nombre de courses\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c3d9d",
   "metadata": {},
   "source": [
    "## 2.5 Analyse Spatiale\n",
    "\n",
    "Examinons la distribution des courses par zone communautaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des zones communautaires (pickup_community_area)\n",
    "if 'pickup_community_area' in df_sample.columns:\n",
    "    # Compter les courses par zone\n",
    "    zone_counts = df_sample['pickup_community_area'].value_counts().reset_index()\n",
    "    zone_counts.columns = ['pickup_community_area', 'count']\n",
    "    zone_counts = zone_counts.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Afficher les 15 principales zones\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.barplot(data=zone_counts.head(15), x='pickup_community_area', y='count', palette=\"viridis\")\n",
    "    plt.title(\"15 Zones Communautaires les Plus Actives (Départs)\")\n",
    "    plt.xlabel(\"Zone Communautaire (pickup_community_area)\")\n",
    "    plt.ylabel(\"Nombre de Courses\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb850fa",
   "metadata": {},
   "source": [
    "# 3. Préparation des Données pour le Forecasting\n",
    "\n",
    "Maintenant que nous avons exploré les données, préparons-les pour le forecasting en les agrégeant par heure et par zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73456d31",
   "metadata": {},
   "source": [
    "## 3.1 Extraction des Données Complètes pour le Forecasting\n",
    "\n",
    "Chargeons les données complètes avec les colonnes essentielles pour le forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requête SQL pour charger les données nécessaires au forecasting\n",
    "forecasting_query = \"\"\"\n",
    "SELECT\n",
    "  trip_start_timestamp,\n",
    "  pickup_community_area\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE \n",
    "  pickup_community_area IS NOT NULL\n",
    "  AND trip_start_timestamp >= '2022-01-01'\n",
    "  AND trip_start_timestamp < '2023-01-01'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Chargement des données depuis BigQuery... (peut prendre plusieurs minutes)\")\n",
    "\n",
    "# Option 1 : Charger via python - peut être lent pour de grands volumes\n",
    "try:\n",
    "    df_raw = read_gbq(forecasting_query, project_id=PROJECT_ID)\n",
    "    print(f\"✅ Dataset chargé avec succès. Nombre de lignes : {len(df_raw):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement depuis BigQuery: {e}\")\n",
    "    print(\"⚠️ Nous allons passer à l'option 2: agrégation directe dans BigQuery\")\n",
    "    df_raw = None\n",
    "\n",
    "# Option 2 (alternative) : Effectuer l'agrégation directement dans BigQuery\n",
    "# Cette option est commentée car elle nécessite des privilèges d'écriture dans BigQuery\n",
    "# Elle sera utilisée plus tard si nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe99143",
   "metadata": {},
   "source": [
    "## 3.2 Agrégation Temporelle par Heure et Zone\n",
    "\n",
    "Agrégeons les données par heure et par zone communautaire pour créer la série temporelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a030e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_raw is not None:\n",
    "    # Convertir les timestamps au format horaire (arrondi à l'heure)\n",
    "    df_raw[\"timestamp_hour\"] = pd.to_datetime(df_raw[\"trip_start_timestamp\"]).dt.floor(\"H\")\n",
    "    \n",
    "    # Agrégation : nombre de courses par heure et par pickup_community_area\n",
    "    df_demand = (\n",
    "        df_raw\n",
    "        .groupby([\"timestamp_hour\", \"pickup_community_area\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"trip_count\")\n",
    "        .sort_values([\"timestamp_hour\", \"pickup_community_area\"])\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Données agrégées : {len(df_demand):,} lignes.\")\n",
    "    df_demand.head()\n",
    "else:\n",
    "    print(\"⚠️ Pas de données brutes disponibles pour l'agrégation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ff07c",
   "metadata": {},
   "source": [
    "## 3.3 Complétion des Séries Temporelles\n",
    "\n",
    "Assurons-nous que toutes les combinaisons d'heures et de zones existent, même s'il n'y a pas de courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eae973",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_demand' in locals() and not df_demand.empty:\n",
    "    # Créer l'ensemble des heures disponibles dans le dataset (par pas de 1 heure)\n",
    "    min_time = df_demand[\"timestamp_hour\"].min()\n",
    "    max_time = df_demand[\"timestamp_hour\"].max()\n",
    "    all_hours = pd.date_range(start=min_time, end=max_time, freq=\"H\")\n",
    "    \n",
    "    # Identifier toutes les zones uniques\n",
    "    all_zones = df_demand[\"pickup_community_area\"].dropna().unique()\n",
    "    all_zones = sorted(all_zones)\n",
    "    \n",
    "    # Créer le produit cartésien : toutes les combinaisons heure × zone\n",
    "    complete_index = pd.MultiIndex.from_product(\n",
    "        [all_hours, all_zones],\n",
    "        names=[\"timestamp_hour\", \"pickup_community_area\"]\n",
    "    )\n",
    "    \n",
    "    # Créer un DataFrame complet\n",
    "    df_complete = pd.DataFrame(index=complete_index).reset_index()\n",
    "    \n",
    "    # Fusionner avec les données observées\n",
    "    df_demand_complete = pd.merge(\n",
    "        df_complete,\n",
    "        df_demand,\n",
    "        on=[\"timestamp_hour\", \"pickup_community_area\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Remplacer les valeurs manquantes (heures sans courses) par 0\n",
    "    df_demand_complete[\"trip_count\"] = df_demand_complete[\"trip_count\"].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"✅ Séries temporelles complétées : {len(df_demand_complete):,} lignes.\")\n",
    "    df_demand_complete.head()\n",
    "else:\n",
    "    print(\"⚠️ Pas de données agrégées disponibles pour la complétion des séries temporelles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec18608",
   "metadata": {},
   "source": [
    "## 3.4 Ingénierie de Features Temporelles\n",
    "\n",
    "Ajoutons des features temporelles utiles pour le forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cf87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_demand_complete' in locals() and not df_demand_complete.empty:\n",
    "    # Extraire les features temporelles classiques\n",
    "    df_demand_complete[\"hour\"] = df_demand_complete[\"timestamp_hour\"].dt.hour\n",
    "    df_demand_complete[\"day_of_week\"] = df_demand_complete[\"timestamp_hour\"].dt.dayofweek  # Lundi = 0\n",
    "    df_demand_complete[\"month\"] = df_demand_complete[\"timestamp_hour\"].dt.month\n",
    "    df_demand_complete[\"day_of_year\"] = df_demand_complete[\"timestamp_hour\"].dt.dayofyear\n",
    "    df_demand_complete[\"week_of_year\"] = df_demand_complete[\"timestamp_hour\"].dt.isocalendar().week.astype(int)\n",
    "    df_demand_complete[\"year\"] = df_demand_complete[\"timestamp_hour\"].dt.year\n",
    "    df_demand_complete[\"is_weekend\"] = df_demand_complete[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Encodage cyclique pour l'heure (utile pour certains modèles)\n",
    "    df_demand_complete[\"hour_sin\"] = np.sin(2 * np.pi * df_demand_complete[\"hour\"] / 24)\n",
    "    df_demand_complete[\"hour_cos\"] = np.cos(2 * np.pi * df_demand_complete[\"hour\"] / 24)\n",
    "    \n",
    "    print(\"✅ Features temporelles ajoutées.\")\n",
    "    # Aperçu\n",
    "    df_demand_complete[[\"timestamp_hour\", \"pickup_community_area\", \"trip_count\", \"hour\", \"day_of_week\", \"month\", \"is_weekend\"]].head()\n",
    "else:\n",
    "    print(\"⚠️ Pas de données disponibles pour l'ajout de features temporelles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618b62a",
   "metadata": {},
   "source": [
    "## 3.5 Visualisation des Séries Temporelles\n",
    "\n",
    "Visualisons les séries temporelles pour quelques zones principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_demand_complete' in locals() and not df_demand_complete.empty:\n",
    "    # Identifier les zones les plus actives\n",
    "    top_active_zones = df_demand_complete.groupby('pickup_community_area')['trip_count'].sum().nlargest(5).index.tolist()\n",
    "    \n",
    "    # Filtrer pour ces zones\n",
    "    df_top_zones = df_demand_complete[df_demand_complete['pickup_community_area'].isin(top_active_zones)]\n",
    "    \n",
    "    # Agréger par jour pour simplifier la visualisation\n",
    "    df_top_zones['date'] = df_top_zones['timestamp_hour'].dt.date\n",
    "    df_daily = df_top_zones.groupby(['date', 'pickup_community_area'])['trip_count'].sum().reset_index()\n",
    "    \n",
    "    # Visualiser les tendances pour les 5 zones principales\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    for zone in top_active_zones:\n",
    "        df_zone = df_daily[df_daily['pickup_community_area'] == zone]\n",
    "        plt.plot(df_zone['date'], df_zone['trip_count'], label=f'Zone {zone}')\n",
    "    \n",
    "    plt.title('Nombre de Courses par Jour - 5 Zones Principales', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Nombre de Courses', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualisation hebdomadaire pour une zone spécifique\n",
    "    # Sélectionner la zone la plus active\n",
    "    top_zone = top_active_zones[0]\n",
    "    \n",
    "    # Analyse par jour de la semaine\n",
    "    df_top_zone = df_demand_complete[df_demand_complete['pickup_community_area'] == top_zone]\n",
    "    weekly_pattern = df_top_zone.groupby(['day_of_week', 'hour'])['trip_count'].mean().reset_index()\n",
    "    weekly_pattern_pivot = weekly_pattern.pivot(index='hour', columns='day_of_week', values='trip_count')\n",
    "    \n",
    "    # Visualisation heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(weekly_pattern_pivot, cmap=\"viridis\", annot=False, fmt=\".1f\")\n",
    "    plt.title(f'Pattern Hebdomadaire Moyen - Zone {top_zone}', fontsize=16)\n",
    "    plt.xlabel('Jour de la Semaine (0=Lundi, 6=Dimanche)', fontsize=14)\n",
    "    plt.ylabel('Heure de la Journée', fontsize=14)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Pas de données disponibles pour la visualisation des séries temporelles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135ea44",
   "metadata": {},
   "source": [
    "## 3.6 Export vers BigQuery pour Vertex AI Forecast\n",
    "\n",
    "Exportons les données préparées vers BigQuery pour l'utilisation avec Vertex AI Forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711065ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_demand_complete' in locals() and not df_demand_complete.empty:\n",
    "    # Alternative: si les données sont trop volumineuses, utiliser la requête SQL suivante directement dans BigQuery\n",
    "    # (cette requête effectue la même transformation que les étapes précédentes)\n",
    "    bq_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{BQ_DATASET}.demand_by_hour` AS\n",
    "    WITH \n",
    "      hours AS (\n",
    "        SELECT \n",
    "          MIN(TIMESTAMP_TRUNC(trip_start_timestamp, HOUR)) AS min_hour,\n",
    "          MAX(TIMESTAMP_TRUNC(trip_start_timestamp, HOUR)) AS max_hour\n",
    "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE \n",
    "          trip_start_timestamp >= '2022-01-01' \n",
    "          AND trip_start_timestamp < '2023-01-01'\n",
    "      ),\n",
    "      all_hours AS (\n",
    "        SELECT timestamp_hour\n",
    "        FROM hours,\n",
    "        UNNEST(GENERATE_TIMESTAMP_ARRAY(min_hour, max_hour, INTERVAL 1 HOUR)) AS timestamp_hour\n",
    "      ),\n",
    "      areas AS (\n",
    "        SELECT DISTINCT pickup_community_area\n",
    "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE pickup_community_area IS NOT NULL\n",
    "      ),\n",
    "      all_combinations AS (\n",
    "        SELECT\n",
    "          h.timestamp_hour,\n",
    "          a.pickup_community_area\n",
    "        FROM all_hours h\n",
    "        CROSS JOIN areas a\n",
    "      ),\n",
    "      aggregated AS (\n",
    "        SELECT\n",
    "          TIMESTAMP_TRUNC(trip_start_timestamp, HOUR) AS timestamp_hour,\n",
    "          pickup_community_area,\n",
    "          COUNT(*) AS trip_count\n",
    "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE \n",
    "          pickup_community_area IS NOT NULL\n",
    "          AND trip_start_timestamp >= '2022-01-01' \n",
    "          AND trip_start_timestamp < '2023-01-01'\n",
    "        GROUP BY 1, 2\n",
    "      ),\n",
    "      filled AS (\n",
    "        SELECT\n",
    "          ac.timestamp_hour,\n",
    "          ac.pickup_community_area,\n",
    "          IFNULL(agg.trip_count, 0) AS trip_count\n",
    "        FROM all_combinations ac\n",
    "        LEFT JOIN aggregated agg\n",
    "          ON ac.timestamp_hour = agg.timestamp_hour\n",
    "         AND ac.pickup_community_area = agg.pickup_community_area\n",
    "      )\n",
    "    SELECT\n",
    "      timestamp_hour,\n",
    "      pickup_community_area,\n",
    "      trip_count,\n",
    "      EXTRACT(HOUR FROM timestamp_hour) AS hour,\n",
    "      EXTRACT(DAYOFWEEK FROM timestamp_hour) AS day_of_week,\n",
    "      EXTRACT(MONTH FROM timestamp_hour) AS month,\n",
    "      EXTRACT(YEAR FROM timestamp_hour) AS year,\n",
    "      EXTRACT(DAYOFYEAR FROM timestamp_hour) AS day_of_year,\n",
    "      IF(EXTRACT(DAYOFWEEK FROM timestamp_hour) IN (1, 7), 1, 0) AS is_weekend\n",
    "    FROM filled\n",
    "    ORDER BY timestamp_hour, pickup_community_area;\n",
    "    \"\"\".format(PROJECT_ID=PROJECT_ID, BQ_DATASET=BQ_DATASET)\n",
    "    \n",
    "    # Exécuter la requête BigQuery\n",
    "    print(\"Exécution de la requête d'agrégation directement dans BigQuery...\")\n",
    "    try:\n",
    "        job = client.query(bq_query)\n",
    "        job.result()  # Attendre la fin du job\n",
    "        print(f\"✅ Table exportée vers BigQuery : {PROJECT_ID}.{BQ_DATASET}.demand_by_hour\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur lors de l'exécution de la requête BigQuery : {e}\")\n",
    "        \n",
    "        # Option alternative: exporter depuis pandas si la requête SQL échoue\n",
    "        try:\n",
    "            print(\"Tentative d'export depuis pandas...\")\n",
    "            # Sélectionner les colonnes pertinentes\n",
    "            columns_to_keep = [\n",
    "                \"timestamp_hour\", \"pickup_community_area\", \"trip_count\",\n",
    "                \"hour\", \"day_of_week\", \"month\", \"year\", \"day_of_year\", \"is_weekend\"\n",
    "            ]\n",
    "            \n",
    "            # Filtrer les colonnes\n",
    "            df_to_export = df_demand_complete[columns_to_keep]\n",
    "            \n",
    "            # Configurer le job\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "            )\n",
    "            \n",
    "            # Exporter\n",
    "            table_id = f\"{PROJECT_ID}.{BQ_DATASET}.demand_by_hour\"\n",
    "            job = client.load_table_from_dataframe(\n",
    "                df_to_export, table_id, job_config=job_config\n",
    "            )\n",
    "            job.result()  # Attendre la fin du job\n",
    "            print(f\"✅ Table exportée vers BigQuery via pandas : {table_id}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ Échec de l'export via pandas : {e2}\")\n",
    "else:\n",
    "    # Si nous n'avons pas pu charger les données en mémoire, exécuter directement la requête\n",
    "    print(\"Pas de données en mémoire, exécution de la requête d'agrégation directement dans BigQuery...\")\n",
    "    # (Code de la requête BigQuery identique à celui ci-dessus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381d741",
   "metadata": {},
   "source": [
    "# 4. Conclusion et Prochaines Étapes\n",
    "\n",
    "## Résumé de l'Analyse\n",
    "- Nous avons exploré le dataset Chicago Taxi Trips et identifié des patterns temporels importants.\n",
    "- Les données ont été agrégées par heure et par zone communautaire (pickup_community_area).\n",
    "- Nous avons créé des features temporelles utiles pour le forecasting.\n",
    "- Les données préparées ont été exportées vers BigQuery pour une utilisation avec Vertex AI Forecast.\n",
    "\n",
    "## Prochaines Étapes\n",
    "- Utiliser Vertex AI Forecast pour entraîner un modèle de prévision (dans le notebook suivant).\n",
    "- Évaluer la performance du modèle.\n",
    "- Générer des prédictions pour les périodes futures.\n",
    "- Visualiser les résultats de prévision pour aider à la prise de décision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
