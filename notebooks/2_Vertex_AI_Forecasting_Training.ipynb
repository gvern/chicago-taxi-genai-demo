{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787efd37",
   "metadata": {},
   "source": [
    "# 2. Modélisation avec Vertex AI Forecast - Chicago Taxi Demand\n",
    "\n",
    "Ce notebook couvre l'entraînement d'un modèle de prévision de la demande de taxis à Chicago en utilisant Vertex AI Forecast. Nous allons :\n",
    "- Initialiser l'environnement Vertex AI\n",
    "- Charger la configuration depuis le fichier YAML\n",
    "- Créer un dataset de séries temporelles à partir des données BigQuery\n",
    "- Configurer et lancer un job d'entraînement AutoML Forecasting\n",
    "- Analyser les résultats et les métriques d'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00870c01",
   "metadata": {},
   "source": [
    "## 1. Configuration et Initialisation\n",
    "\n",
    "Importons les bibliothèques nécessaires et initialisons l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ce17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques standards\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Google Cloud & Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Configuration visuelle\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2669cd7",
   "metadata": {},
   "source": [
    "## 2. Configuration du Projet GCP\n",
    "\n",
    "Définissons les identifiants du projet et initialisons Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8394037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration chargée avec succès depuis pipeline_config.yaml\n",
      "Using PROJECT_ID: avisia-certification-ml-yde, REGION: europe-west1, BQ_DATASET: chicago_taxis\n",
      "✅ Vertex AI initialisé avec succès pour le projet avisia-certification-ml-yde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client BigQuery initialisé pour le projet avisia-certification-ml-yde en région europe-west1\n"
     ]
    }
   ],
   "source": [
    "# Configuration du projet GCP\n",
    "# Load from YAML first\n",
    "try:\n",
    "    with open(\"../config/pipeline_config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration chargée avec succès depuis pipeline_config.yaml\")\n",
    "    PROJECT_ID = config.get('gcp', {}).get('project_id', 'avisia-certification-ml-yde') \n",
    "    REGION = config.get('gcp', {}).get('region', 'europe-west1')\n",
    "    BQ_DATASET = config.get('bigquery', {}).get('dataset_id', 'chicago_taxis')  \n",
    "    BUCKET_URI = config.get('gcp', {}).get('bucket_uri', f\"gs://{PROJECT_ID}-vertex-bucket\")  \n",
    "    BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.demand_by_hour\"\n",
    "    # Add defaults if keys might be missing\n",
    "except Exception as e:\n",
    "    print(f\"Impossible de charger le fichier de configuration: {e}. Using default values.\")\n",
    "    config = {}\n",
    "    PROJECT_ID = \"avisia-certification-ml-yde\" # Fallback\n",
    "    REGION = \"europe-west1\" # Fallback\n",
    "    BQ_DATASET = \"chicago_taxis\" # Fallback\n",
    "\n",
    "print(f\"Using PROJECT_ID: {PROJECT_ID}, REGION: {REGION}, BQ_DATASET: {BQ_DATASET}\")\n",
    "# Initialisation de Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"✅ Vertex AI initialisé avec succès pour le projet {PROJECT_ID}\")\n",
    "# Initialisation du client BigQuery\n",
    "try:\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=REGION) # Spécifier location peut aider\n",
    "    print(f\"✅ Client BigQuery initialisé pour le projet {PROJECT_ID} en région {REGION}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'initialisation du client BigQuery: {e}\")\n",
    "    # Gérer l'erreur ou arrêter si le client est indispensable\n",
    "    client = None # Ou raise SystemExit(\"Client BigQuery requis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbece398",
   "metadata": {},
   "source": [
    "## 3. Chargement de la Configuration\n",
    "\n",
    "Chargeons les paramètres de configuration depuis le fichier YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2b9e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée avec succès.\n",
      "\n",
      "Paramètres de forecasting:\n",
      "- Colonne temporelle: timestamp_hour\n",
      "- Colonne cible: trip_count\n",
      "- Identifiant de série: pickup_community_area\n",
      "- Horizon de prévision: 24 heures\n",
      "- Taille de la fenêtre historique: 168 heures\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier de configuration\n",
    "try:\n",
    "    with open(\"../config/pipeline_config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"✅ Configuration chargée avec succès.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        with open(\"config/pipeline_config.yaml\", \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(\"✅ Configuration chargée avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ Fichier de configuration introuvable. Utilisation des valeurs par défaut.\")\n",
    "        config = {\n",
    "            \"forecasting\": {\n",
    "                \"time_column\": \"timestamp_hour\",\n",
    "                \"target_column\": \"trip_count\",\n",
    "                \"context_column\": \"pickup_community_area\",\n",
    "                \"forecast_horizon\": 24,\n",
    "                \"window_size\": 168,\n",
    "                \"available_at_forecast\": [\n",
    "                    \"timestamp_hour\", \"day_of_year\", \"day_of_week\", \"hour\", \n",
    "                    \"month\", \"is_weekend\"\n",
    "                ],\n",
    "                \"unavailable_at_forecast\": [\"trip_count\"],\n",
    "                \"data_granularity_unit\": \"hour\"\n",
    "            },\n",
    "            \"vertex_ai_forecast\": {\n",
    "                \"display_name\": \"chicago_taxi_forecast_model\",\n",
    "                \"optimization_objective\": \"minimize-rmse\",\n",
    "                \"budget_milli_node_hours\": 100\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Extraction des paramètres de configuration\n",
    "forecast_config = config[\"forecasting\"]\n",
    "vertex_config = config[\"vertex_ai_forecast\"]\n",
    "time_window = forecast_config[\"time_window\"]\n",
    "max_data_points = time_window[\"max_data_points\"]\n",
    "if max_data_points is None:\n",
    "    max_data_points = 2950\n",
    "end_date = time_window[\"end_date\"] \n",
    "end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "start_date = end_date - timedelta(hours=max_data_points)\n",
    "# Paramètres clés pour le forecasting\n",
    "time_column = forecast_config[\"time_column\"]\n",
    "target_column = forecast_config[\"target_column\"]\n",
    "context_column = forecast_config[\"context_column\"]\n",
    "forecast_horizon = forecast_config[\"forecast_horizon\"]\n",
    "window_size = forecast_config[\"window_size\"]\n",
    "available_at_forecast = forecast_config[\"available_at_forecast\"]\n",
    "unavailable_at_forecast = forecast_config[\"unavailable_at_forecast\"]\n",
    "\n",
    "# Affichage des principaux paramètres\n",
    "print(f\"\\nParamètres de forecasting:\")\n",
    "print(f\"- Colonne temporelle: {time_column}\")\n",
    "print(f\"- Colonne cible: {target_column}\")\n",
    "print(f\"- Identifiant de série: {context_column}\")\n",
    "print(f\"- Horizon de prévision: {forecast_horizon} heures\")\n",
    "print(f\"- Taille de la fenêtre historique: {window_size} heures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75e3f0",
   "metadata": {},
   "source": [
    "## 4. Création du Dataset de Séries Temporelles\n",
    "\n",
    " Créons un dataset de séries temporelles dans Vertex AI à partir de la table BigQuery\n",
    " `demand_by_hour` préparée et filtrée par le Notebook 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8534259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TimeSeriesDataset\n",
      "Create TimeSeriesDataset backing LRO: projects/807699310940/locations/europe-west1/datasets/4187740723036028928/operations/3513644162819817472\n",
      "Create TimeSeriesDataset backing LRO: projects/807699310940/locations/europe-west1/datasets/4187740723036028928/operations/3513644162819817472\n",
      "TimeSeriesDataset created. Resource name: projects/807699310940/locations/europe-west1/datasets/4187740723036028928\n",
      "To use this TimeSeriesDataset in another session:\n",
      "ds = aiplatform.TimeSeriesDataset('projects/807699310940/locations/europe-west1/datasets/4187740723036028928')\n",
      "✅ Dataset créé avec données filtrées: projects/807699310940/locations/europe-west1/datasets/4187740723036028928\n"
     ]
    }
   ],
   "source": [
    "# Nom d'affichage du dataset\n",
    "dataset_display_name = f\"{vertex_config['display_name']}-dataset-{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# Vérifier que BQ_SOURCE_URI est défini (devrait venir du début du notebook ou du config)\n",
    "if 'BQ_SOURCE_URI' not in locals():\n",
    "    # Recalculer BQ_SOURCE_URI à partir de PROJECT_ID et BQ_DATASET si nécessaire\n",
    "    BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.demand_by_hour\"\n",
    "    print(f\"BQ_SOURCE_URI défini à : {BQ_SOURCE_URI}\")\n",
    "print(f\"Utilisation de la source BigQuery: {BQ_SOURCE_URI}\")\n",
    "\n",
    "\n",
    "# Création du dataset avec la table filtrée\n",
    "try:\n",
    "    # Utiliser la table BigQuery filtrée au lieu de la table entière\n",
    "    dataset = aiplatform.TimeSeriesDataset.create(\n",
    "        display_name=dataset_display_name,\n",
    "        bq_source=BQ_SOURCE_URI,  # Utiliser la table filtrée\n",
    "    )\n",
    "    print(f\"✅ Dataset créé avec données filtrées: {dataset.resource_name}\")\n",
    "    print(f\"Lien vers le dataset dans la console Cloud:\")\n",
    "    print(f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/datasets/{dataset.name}?project={PROJECT_ID}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erreur lors de la création du dataset: {e}\")\n",
    "    print(\"Tentative de récupération d'un dataset existant...\")\n",
    "    try:\n",
    "        # Rechercher parmi les datasets existants\n",
    "        datasets = aiplatform.TimeSeriesDataset.list(\n",
    "            filter=f'display_name=\"{dataset_display_name}\"',\n",
    "            order_by=\"create_time desc\",\n",
    "            location=REGION,\n",
    "            project=PROJECT_ID\n",
    "        )\n",
    "        if datasets:\n",
    "            dataset = datasets[0]\n",
    "            print(f\"✅ Dataset existant récupéré: {dataset.resource_name}\")\n",
    "            print(f\"Lien: https://console.cloud.google.com/vertex-ai/locations/{REGION}/datasets/{dataset.name}?project={PROJECT_ID}\")\n",
    "            # Ajouter une vérification des données si possible/nécessaire\n",
    "            print(f\"   Source BQ associée: {getattr(dataset, 'metadata', {}).get('inputConfig', {}).get('bigquerySource', {}).get('uri', 'Non trouvée')}\")\n",
    "            print(f\"⚠️ ATTENTION: Ce dataset pourrait contenir plus de 3000 points par série!\")\n",
    "        else:\n",
    "            print(f\"❌ Aucun dataset existant trouvé avec le nom : {dataset_display_name}\")\n",
    "            dataset = None # Assurer que dataset est None si échec\n",
    "\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Erreur lors de la récupération du dataset: {e2}\")\n",
    "# Vérifier si le dataset a bien été créé ou récupéré\n",
    "if not dataset:\n",
    "    print(\"❌ Impossible de continuer sans dataset Vertex AI.\")\n",
    "    # Vous pourriez vouloir arrêter l'exécution ici: \n",
    "    raise SystemExit(\"Erreur critique: Dataset non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf126c",
   "metadata": {},
   "source": [
    "## 5. Configuration et Lancement du Job d'Entraînement\n",
    "\n",
    "Configurons et lançons un job d'entraînement AutoML Forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac15540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations de colonnes configurées: 7 colonnes\n",
      "Note: pickup_community_area (identifiant de série) est EXCLU des transformations pour éviter les conflits\n"
     ]
    }
   ],
   "source": [
    "# Nom du job d'entraînement\n",
    "job_display_name = f\"taxi_demand_forecast_job_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# Configuration des transformations de colonnes\n",
    "# Définir explicitement les transformations pour chaque colonne sauf pickup_community_area\n",
    "formatted_transformations = [\n",
    "    {\"timestamp\": {\"column_name\": time_column}},  # La colonne timestamp\n",
    "    {\"numeric\": {\"column_name\": target_column}},   # La colonne cible\n",
    "    \n",
    "    # Features supplémentaires disponibles à l'heure de prévision\n",
    "    {\"numeric\": {\"column_name\": \"day_of_year\"}},\n",
    "    {\"numeric\": {\"column_name\": \"day_of_week\"}},\n",
    "    {\"numeric\": {\"column_name\": \"hour\"}},\n",
    "    {\"numeric\": {\"column_name\": \"month\"}},\n",
    "    {\"categorical\": {\"column_name\": \"is_weekend\"}}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Transformations de colonnes configurées: {len(formatted_transformations)} colonnes\")\n",
    "print(f\"Note: {context_column} (identifiant de série) est EXCLU des transformations pour éviter les conflits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d356e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Job d'entraînement configuré: taxi_demand_forecast_job_20250415_1103\n"
     ]
    }
   ],
   "source": [
    "# Création du job d'entraînement\n",
    "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "    display_name=job_display_name,\n",
    "    optimization_objective=vertex_config.get(\"optimization_objective\", \"minimize-rmse\"),\n",
    "    column_transformations=formatted_transformations,\n",
    ")\n",
    "\n",
    "print(f\"✅ Job d'entraînement configuré: {job_display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ea34f",
   "metadata": {},
   "source": [
    "## Dataset Splitting Justification\n",
    "\n",
    "Pour respecter la temporalité (éviter les fuites), nous avons fait :\n",
    "- 80% des dates les plus anciennes pour le train\n",
    "- 10% (période intermédiaire) pour la validation\n",
    "- 10% (période la plus récente) pour le test\n",
    "\n",
    "Cela reflète un scénario réaliste où l'on prévoit sur des dates futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9344df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Démarrage de l'entraînement du modèle... Cela peut prendre plusieurs heures.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/3734390157390905344?project=807699310940\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "AutoMLForecastingTrainingJob projects/807699310940/locations/europe-west1/trainingPipelines/3734390157390905344 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "❌ Erreur lors de l'entraînement du modèle: Training failed with:\n",
      "code: 1\n",
      "message: \"CANCELED\"\n",
      "\n",
      "\n",
      "Détails complets de l'erreur pour analyse:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/yz/8n0rd9yn0hz34drfl1h86v300000gn/T/ipykernel_53686/1537265053.py\", line 16, in <module>\n",
      "    model = training_job.run(\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 2204, in run\n",
      "    return self._run(\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/base.py\", line 863, in wrapper\n",
      "    return method(*args, **kwargs)\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 2658, in _run\n",
      "    new_model = self._run_job(\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 854, in _run_job\n",
      "    model = self._get_model(block=block)\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 941, in _get_model\n",
      "    self._block_until_complete()\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 984, in _block_until_complete\n",
      "    self._raise_failure()\n",
      "  File \"/Users/gustavevernay/Desktop/Projets/Pro/Avisia/chicago-taxi-genai-demo/venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py\", line 1001, in _raise_failure\n",
      "    raise RuntimeError(\"Training failed with:\\n%s\" % self._gca_resource.error)\n",
      "RuntimeError: Training failed with:\n",
      "code: 1\n",
      "message: \"CANCELED\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nom du modèle\n",
    "model_display_name = f\"{vertex_config['display_name']}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# S'assurer que context_column n'est PAS dans available_at_forecast ni unavailable_at_forecast\n",
    "available_at_forecast_cleaned = [col for col in available_at_forecast if col != context_column]\n",
    "unavailable_at_forecast_cleaned = [col for col in unavailable_at_forecast if col != context_column]\n",
    "\n",
    "# Informer l'utilisateur des modifications\n",
    "if context_column in available_at_forecast or context_column in unavailable_at_forecast:\n",
    "    print(f\"ℹ️ Note: {context_column} a été retiré des listes de colonnes car c'est l'identifiant de série temporelle.\")\n",
    "    print(f\"L'identifiant de série ne doit pas apparaître dans les listes des colonnes disponibles ou indisponibles.\")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "print(f\"⏳ Démarrage de l'entraînement du modèle... Cela peut prendre plusieurs heures.\")\n",
    "try:\n",
    "    model = training_job.run(\n",
    "        dataset=dataset,\n",
    "        target_column=target_column,\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_column=context_column,\n",
    "        # Utiliser les listes nettoyées sans l'identifiant de série\n",
    "        unavailable_at_forecast_columns=unavailable_at_forecast_cleaned,\n",
    "        available_at_forecast_columns=available_at_forecast_cleaned,\n",
    "        # L'identifiant de série n'a pas besoin d'être spécifié comme attribut de série\n",
    "        time_series_attribute_columns=[],\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        context_window=window_size,\n",
    "        data_granularity_unit=forecast_config.get(\"data_granularity_unit\", \"hour\"),\n",
    "        data_granularity_count=1,\n",
    "        budget_milli_node_hours=vertex_config.get(\"budget_milli_node_hours\", 100),\n",
    "        model_display_name=model_display_name,\n",
    "        training_fraction_split=vertex_config.get(\"training_fraction_split\", 0.8),\n",
    "        validation_fraction_split=vertex_config.get(\"validation_fraction_split\", 0.1),\n",
    "        test_fraction_split=vertex_config.get(\"test_fraction_split\", 0.1),\n",
    "        export_evaluated_data_items=True,\n",
    "        sync=True,  # Mode synchrone: attend la fin de l'entraînement\n",
    "    )\n",
    "    print(f\"✅ Modèle entraîné avec succès: {model.display_name}\")\n",
    "    print(f\"Resource name: {model.resource_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de l'entraînement du modèle: {e}\")\n",
    "    print(\"\\nDétails complets de l'erreur pour analyse:\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716d856",
   "metadata": {},
   "source": [
    "## 6. Évaluation du Modèle\n",
    "\n",
    "Examinons les métriques d'évaluation du modèle entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb813ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de récupérer les métriques d'évaluation: name 'model' is not defined\n",
      "Les métriques peuvent ne pas être disponibles immédiatement après l'entraînement.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Récupération des métriques d'évaluation\n",
    "    evaluation = model.get_model_evaluation()\n",
    "    metrics = evaluation.metrics\n",
    "    \n",
    "    print(\"Métriques d'évaluation :\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if isinstance(metric_value, (int, float)):\n",
    "            print(f\"- {metric_name}: {metric_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"- {metric_name}: {metric_value}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de récupérer les métriques d'évaluation: {e}\")\n",
    "    print(\"Les métriques peuvent ne pas être disponibles immédiatement après l'entraînement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb475b8",
   "metadata": {},
   "source": [
    "## 7. Importance des Features\n",
    "\n",
    "Analysons l'importance des features dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7184700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de récupérer l'importance des features: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Récupération de l'importance des features\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    \n",
    "    if feature_importance:\n",
    "        # Conversion en DataFrame pour faciliter la visualisation\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': [item.feature_id for item in feature_importance],\n",
    "            'Importance': [item.importance_score for item in feature_importance]\n",
    "        })\n",
    "        \n",
    "        # Tri par importance décroissante\n",
    "        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Affichage\n",
    "        print(\"Importance des features :\")\n",
    "        print(feature_importance_df)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "        plt.title('Importance des Features', fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "        plt.ylabel('Feature', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Aucune information d'importance des features disponible.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de récupérer l'importance des features: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426bcad",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des Informations du Modèle\n",
    "\n",
    "Sauvegardons les informations du modèle pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de sauvegarder les informations du modèle: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des informations du modèle dans un fichier pour référence ultérieure\n",
    "try:\n",
    "    model_info = {\n",
    "        'model_name': model.display_name,\n",
    "        'resource_name': model.resource_name,\n",
    "        'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'project': PROJECT_ID,\n",
    "        'region': REGION,\n",
    "        'forecast_horizon': forecast_horizon,\n",
    "        'window_size': window_size,\n",
    "    }\n",
    "    \n",
    "    # Création du répertoire si nécessaire\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarde dans un fichier YAML\n",
    "    with open(f'outputs/model_info_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.yaml', 'w') as f:\n",
    "        yaml.dump(model_info, f)\n",
    "    \n",
    "    print(f\"✅ Informations du modèle sauvegardées.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de sauvegarder les informations du modèle: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced938f8",
   "metadata": {},
   "source": [
    "## 10. Informations de Dépannage\n",
    "\n",
    "### Erreurs courantes et solutions\n",
    "\n",
    "1. **Erreur**: `Found time series with more than 3000 time steps`\n",
    "   - **Solution**: Nous avons ajouté une logique pour limiter la longueur des séries temporelles en ajustant la plage de dates. Si vous rencontrez toujours cette erreur, réduisez davantage la plage de dates.\n",
    "\n",
    "2. **Erreur**: `The columns 'pickup_community_area' have transformation but do not have any time dependency properties`\n",
    "   - **Solution**: Nous avons exclu l'identifiant de série (pickup_community_area) des transformations et des listes de colonnes.\n",
    "\n",
    "3. **Performances du modèle faibles**:\n",
    "   - **Solution**: Vous pouvez augmenter `budget_milli_node_hours` pour permettre à AutoML d'explorer plus de modèles.\n",
    "   - Essayez d'ajouter plus de variables contextuelles ou météorologiques.\n",
    "   - Ajustez l'horizon de prévision et la taille de la fenêtre de contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b62425",
   "metadata": {},
   "source": [
    "## 9. Conclusion et Prochaines Étapes\n",
    "\n",
    "### Résumé\n",
    "- Nous avons créé un dataset de séries temporelles à partir des données BigQuery\n",
    "- Nous avons configuré et entraîné un modèle de forecasting avec Vertex AI AutoML\n",
    "- Nous avons évalué les performances du modèle et analysé l'importance des features\n",
    "\n",
    "### Prochaines Étapes\n",
    "- Utiliser le modèle pour générer des prédictions batch sur des périodes futures\n",
    "- Visualiser et analyser les résultats de prévision\n",
    "- Explorer d'autres configurations de modèle pour améliorer les performances\n",
    "\n",
    "Le notebook suivant (3_Batch_Prediction_Visualization.ipynb) abordera la génération et la visualisation des prédictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
