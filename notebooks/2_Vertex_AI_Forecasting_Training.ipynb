{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787efd37",
   "metadata": {},
   "source": [
    "# 2. Modélisation avec Vertex AI Forecast - Chicago Taxi Demand\n",
    "\n",
    "Ce notebook couvre l'entraînement d'un modèle de prévision de la demande de taxis à Chicago en utilisant Vertex AI Forecast. Nous allons :\n",
    "- Initialiser l'environnement Vertex AI\n",
    "- Charger la configuration depuis le fichier YAML\n",
    "- Créer un dataset de séries temporelles à partir des données BigQuery\n",
    "- Configurer et lancer un job d'entraînement AutoML Forecasting\n",
    "- Analyser les résultats et les métriques d'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00870c01",
   "metadata": {},
   "source": [
    "## 1. Configuration et Initialisation\n",
    "\n",
    "Importons les bibliothèques nécessaires et initialisons l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ce17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques standards\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Google Cloud & Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Configuration visuelle\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2669cd7",
   "metadata": {},
   "source": [
    "## 2. Configuration du Projet GCP\n",
    "\n",
    "Définissons les identifiants du projet et initialisons Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8394037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vertex AI initialisé avec succès pour le projet avisia-certification-ml-yde\n"
     ]
    }
   ],
   "source": [
    "# Configuration du projet GCP\n",
    "PROJECT_ID = \"avisia-certification-ml-yde\"  # Remplacez par votre Project ID\n",
    "REGION = \"europe-west1\"  # Région pour Vertex AI (assurez-vous qu'elle supporte Forecast)\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-vertex-bucket\"  # URI du bucket GCS\n",
    "BQ_DATASET = \"chicago_taxis\"\n",
    "BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.demand_by_hour\"\n",
    "\n",
    "# Initialisation de Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"✅ Vertex AI initialisé avec succès pour le projet {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbece398",
   "metadata": {},
   "source": [
    "## 3. Chargement de la Configuration\n",
    "\n",
    "Chargeons les paramètres de configuration depuis le fichier YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2b9e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration chargée avec succès.\n",
      "\n",
      "Paramètres de forecasting:\n",
      "- Colonne temporelle: timestamp_hour\n",
      "- Colonne cible: trip_count\n",
      "- Identifiant de série: pickup_community_area\n",
      "- Horizon de prévision: 24 heures\n",
      "- Taille de la fenêtre historique: 168 heures\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier de configuration\n",
    "try:\n",
    "    with open(\"../config/pipeline_config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"✅ Configuration chargée avec succès.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        with open(\"config/pipeline_config.yaml\", \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(\"✅ Configuration chargée avec succès.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ Fichier de configuration introuvable. Utilisation des valeurs par défaut.\")\n",
    "        config = {\n",
    "            \"forecasting\": {\n",
    "                \"time_column\": \"timestamp_hour\",\n",
    "                \"target_column\": \"trip_count\",\n",
    "                \"context_column\": \"pickup_community_area\",\n",
    "                \"forecast_horizon\": 24,\n",
    "                \"window_size\": 168,\n",
    "                \"available_at_forecast\": [\n",
    "                    \"timestamp_hour\", \"day_of_year\", \"day_of_week\", \"hour\", \n",
    "                    \"month\", \"is_weekend\"\n",
    "                ],\n",
    "                \"unavailable_at_forecast\": [\"trip_count\"],\n",
    "                \"data_granularity_unit\": \"hour\"\n",
    "            },\n",
    "            \"vertex_ai_forecast\": {\n",
    "                \"display_name\": \"chicago_taxi_forecast_model\",\n",
    "                \"optimization_objective\": \"minimize-rmse\",\n",
    "                \"budget_milli_node_hours\": 100\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Extraction des paramètres de configuration\n",
    "forecast_config = config[\"forecasting\"]\n",
    "vertex_config = config[\"vertex_ai_forecast\"]\n",
    "\n",
    "# Paramètres clés pour le forecasting\n",
    "time_column = forecast_config[\"time_column\"]\n",
    "target_column = forecast_config[\"target_column\"]\n",
    "context_column = forecast_config[\"context_column\"]\n",
    "forecast_horizon = forecast_config[\"forecast_horizon\"]\n",
    "window_size = forecast_config[\"window_size\"]\n",
    "available_at_forecast = forecast_config[\"available_at_forecast\"]\n",
    "unavailable_at_forecast = forecast_config[\"unavailable_at_forecast\"]\n",
    "\n",
    "# Affichage des principaux paramètres\n",
    "print(f\"\\nParamètres de forecasting:\")\n",
    "print(f\"- Colonne temporelle: {time_column}\")\n",
    "print(f\"- Colonne cible: {target_column}\")\n",
    "print(f\"- Identifiant de série: {context_column}\")\n",
    "print(f\"- Horizon de prévision: {forecast_horizon} heures\")\n",
    "print(f\"- Taille de la fenêtre historique: {window_size} heures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75e3f0",
   "metadata": {},
   "source": [
    "## 4. Création du Dataset de Séries Temporelles\n",
    "\n",
    "Créons un dataset de séries temporelles dans Vertex AI à partir de la table BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8534259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TimeSeriesDataset\n",
      "Create TimeSeriesDataset backing LRO: projects/807699310940/locations/europe-west1/datasets/7446095053438582784/operations/679736505110888448\n",
      "Create TimeSeriesDataset backing LRO: projects/807699310940/locations/europe-west1/datasets/7446095053438582784/operations/679736505110888448\n",
      "TimeSeriesDataset created. Resource name: projects/807699310940/locations/europe-west1/datasets/7446095053438582784\n",
      "To use this TimeSeriesDataset in another session:\n",
      "ds = aiplatform.TimeSeriesDataset('projects/807699310940/locations/europe-west1/datasets/7446095053438582784')\n",
      "✅ Dataset créé: projects/807699310940/locations/europe-west1/datasets/7446095053438582784\n"
     ]
    }
   ],
   "source": [
    "# Nom d'affichage du dataset\n",
    "dataset_display_name = f\"{vertex_config['display_name']}-dataset\"\n",
    "\n",
    "# Création du dataset\n",
    "try:\n",
    "    dataset = aiplatform.TimeSeriesDataset.create(\n",
    "        display_name=dataset_display_name,\n",
    "        bq_source=BQ_SOURCE_URI,\n",
    "    )\n",
    "    print(f\"✅ Dataset créé: {dataset.resource_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erreur lors de la création du dataset: {e}\")\n",
    "    print(\"Tentative de récupération d'un dataset existant...\")\n",
    "    try:\n",
    "        # Rechercher parmi les datasets existants\n",
    "        datasets = aiplatform.TimeSeriesDataset.list(\n",
    "            filter=f\"display_name={dataset_display_name}\",\n",
    "            order_by=\"create_time desc\"\n",
    "        )\n",
    "        if datasets:\n",
    "            dataset = datasets[0]\n",
    "            print(f\"✅ Dataset existant récupéré: {dataset.resource_name}\")\n",
    "        else:\n",
    "            print(\"❌ Aucun dataset existant trouvé.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Erreur lors de la récupération du dataset: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf126c",
   "metadata": {},
   "source": [
    "## 5. Configuration et Lancement du Job d'Entraînement\n",
    "\n",
    "Configurons et lançons un job d'entraînement AutoML Forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations de colonnes configurées: 11 colonnes\n"
     ]
    }
   ],
   "source": [
    "# Nom du job d'entraînement\n",
    "job_display_name = f\"taxi_demand_forecast_job_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# Configuration des transformations de colonnes\n",
    "column_transformations = vertex_config.get(\"column_transformations\", {})\n",
    "\n",
    "# Définition MANUELLE des transformations pour éviter les problèmes avec pickup_community_area\n",
    "formatted_transformations = []\n",
    "\n",
    "# Si les transformations sont définies dans la configuration, les utiliser\n",
    "if column_transformations:\n",
    "    # Conversion du format dict en format attendu par l'API\n",
    "    for col, transform_type in column_transformations.items():\n",
    "        # Ne pas ajouter de transformation pour context_column (pickup_community_area)\n",
    "        if col != context_column:\n",
    "            formatted_transformations.append({transform_type: {\"column_name\": col}})\n",
    "else:\n",
    "    # Transformations automatiques pour les colonnes autres que context_column\n",
    "    formatted_transformations.append({\"auto\": {\"column_name\": target_column}})\n",
    "    formatted_transformations.append({\"auto\": {\"column_name\": time_column}})\n",
    "    \n",
    "    # Ajouter les colonnes disponibles à l'heure de la prévision (sauf context_column)\n",
    "    for col in available_at_forecast:\n",
    "        if col not in [target_column, time_column, context_column]:\n",
    "            formatted_transformations.append({\"auto\": {\"column_name\": col}})\n",
    "\n",
    "print(f\"Transformations de colonnes configurées: {len(formatted_transformations)} colonnes\")\n",
    "print(f\"Note: {context_column} est EXCLU des transformations pour éviter les conflits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d356e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Job d'entraînement configuré: taxi_demand_forecast_job_20250409_1736\n"
     ]
    }
   ],
   "source": [
    "# Création du job d'entraînement\n",
    "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "    display_name=job_display_name,\n",
    "    optimization_objective=vertex_config.get(\"optimization_objective\", \"minimize-rmse\"),\n",
    "    column_transformations=formatted_transformations,\n",
    ")\n",
    "\n",
    "print(f\"✅ Job d'entraînement configuré: {job_display_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrait de pickup_community_area des colonnes disponibles pour la prévision\n",
      "⏳ Démarrage de l'entraînement du modèle... Cela peut prendre plusieurs heures.\n",
      "❌ Erreur lors de l'entraînement du modèle: 400 The columns 'pickup_community_area' have transformation but do not have any time dependency properties (time series attribute, unavailable at forecast, available at forecast).\n"
     ]
    }
   ],
   "source": [
    "# Nom du modèle\n",
    "model_display_name = f\"{vertex_config['display_name']}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# S'assurer que context_column n'est PAS dans available_at_forecast ni unavailable_at_forecast\n",
    "if context_column in available_at_forecast:\n",
    "    available_at_forecast.remove(context_column)\n",
    "    print(f\"Retrait de {context_column} des colonnes disponibles pour la prévision\")\n",
    "\n",
    "if context_column in unavailable_at_forecast:\n",
    "    unavailable_at_forecast.remove(context_column)\n",
    "    print(f\"Retrait de {context_column} des colonnes indisponibles pour la prévision\")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "print(f\"⏳ Démarrage de l'entraînement du modèle... Cela peut prendre plusieurs heures.\")\n",
    "try:\n",
    "    model = training_job.run(\n",
    "        dataset=dataset,\n",
    "        target_column=target_column,\n",
    "        time_column=time_column,\n",
    "        time_series_identifier_column=context_column,\n",
    "        unavailable_at_forecast_columns=unavailable_at_forecast,\n",
    "        available_at_forecast_columns=available_at_forecast,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        context_window=window_size,\n",
    "        data_granularity_unit=forecast_config.get(\"data_granularity_unit\", \"hour\"),\n",
    "        data_granularity_count=1,\n",
    "        budget_milli_node_hours=vertex_config.get(\"budget_milli_node_hours\", 100),\n",
    "        model_display_name=model_display_name,\n",
    "        training_fraction_split=vertex_config.get(\"training_fraction_split\", 0.8),\n",
    "        validation_fraction_split=vertex_config.get(\"validation_fraction_split\", 0.1),\n",
    "        test_fraction_split=vertex_config.get(\"test_fraction_split\", 0.1),\n",
    "        export_evaluated_data_items=True,\n",
    "        sync=True,  # Mode synchrone: attend la fin de l'entraînement\n",
    "    )\n",
    "    print(f\"✅ Modèle entraîné avec succès: {model.display_name}\")\n",
    "    print(f\"Resource name: {model.resource_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de l'entraînement du modèle: {e}\")\n",
    "    print(\"\\nDétails complets de l'erreur pour analyse:\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716d856",
   "metadata": {},
   "source": [
    "## 6. Évaluation du Modèle\n",
    "\n",
    "Examinons les métriques d'évaluation du modèle entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb813ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de récupérer les métriques d'évaluation: name 'model' is not defined\n",
      "Les métriques peuvent ne pas être disponibles immédiatement après l'entraînement.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Récupération des métriques d'évaluation\n",
    "    evaluation = model.get_model_evaluation()\n",
    "    metrics = evaluation.metrics\n",
    "    \n",
    "    print(\"Métriques d'évaluation :\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        if isinstance(metric_value, (int, float)):\n",
    "            print(f\"- {metric_name}: {metric_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"- {metric_name}: {metric_value}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de récupérer les métriques d'évaluation: {e}\")\n",
    "    print(\"Les métriques peuvent ne pas être disponibles immédiatement après l'entraînement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb475b8",
   "metadata": {},
   "source": [
    "## 7. Importance des Features\n",
    "\n",
    "Analysons l'importance des features dans le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7184700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de récupérer l'importance des features: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Récupération de l'importance des features\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    \n",
    "    if feature_importance:\n",
    "        # Conversion en DataFrame pour faciliter la visualisation\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': [item.feature_id for item in feature_importance],\n",
    "            'Importance': [item.importance_score for item in feature_importance]\n",
    "        })\n",
    "        \n",
    "        # Tri par importance décroissante\n",
    "        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Affichage\n",
    "        print(\"Importance des features :\")\n",
    "        print(feature_importance_df)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "        plt.title('Importance des Features', fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "        plt.ylabel('Feature', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Aucune information d'importance des features disponible.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de récupérer l'importance des features: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426bcad",
   "metadata": {},
   "source": [
    "## 8. Sauvegarde des Informations du Modèle\n",
    "\n",
    "Sauvegardons les informations du modèle pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d7251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Impossible de sauvegarder les informations du modèle: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des informations du modèle dans un fichier pour référence ultérieure\n",
    "try:\n",
    "    model_info = {\n",
    "        'model_name': model.display_name,\n",
    "        'resource_name': model.resource_name,\n",
    "        'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'project': PROJECT_ID,\n",
    "        'region': REGION,\n",
    "        'forecast_horizon': forecast_horizon,\n",
    "        'window_size': window_size,\n",
    "    }\n",
    "    \n",
    "    # Création du répertoire si nécessaire\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Sauvegarde dans un fichier YAML\n",
    "    with open(f'outputs/model_info_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.yaml', 'w') as f:\n",
    "        yaml.dump(model_info, f)\n",
    "    \n",
    "    print(f\"✅ Informations du modèle sauvegardées.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de sauvegarder les informations du modèle: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b62425",
   "metadata": {},
   "source": [
    "## 9. Conclusion et Prochaines Étapes\n",
    "\n",
    "### Résumé\n",
    "- Nous avons créé un dataset de séries temporelles à partir des données BigQuery\n",
    "- Nous avons configuré et entraîné un modèle de forecasting avec Vertex AI AutoML\n",
    "- Nous avons évalué les performances du modèle et analysé l'importance des features\n",
    "\n",
    "### Prochaines Étapes\n",
    "- Utiliser le modèle pour générer des prédictions batch sur des périodes futures\n",
    "- Visualiser et analyser les résultats de prévision\n",
    "- Explorer d'autres configurations de modèle pour améliorer les performances\n",
    "\n",
    "Le notebook suivant (3_Batch_Prediction_Visualization.ipynb) abordera la génération et la visualisation des prédictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
